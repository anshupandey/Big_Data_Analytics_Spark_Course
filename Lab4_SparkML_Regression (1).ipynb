{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab 4 : Spark ML for Machine Learning (Regression)"
      ],
      "metadata": {
        "id": "Y3Fog9IU29e_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tasks\n",
        "\n",
        "- Installation and Configuration\n",
        "Installs required dependencies, including:\n",
        "- openjdk-8-jdk for Java support.\n",
        "- Spark binaries from Apache Spark's official source.\n",
        "- Python libraries like pyspark and findspark.\n",
        "- Configures environment variables for Java and Spark.\n",
        "\n",
        "- Spark Session Initialization\n",
        "Initializes a Spark session.\n",
        "\n",
        "- Dataset Preparation\n",
        "- Creates a simulated dataset related to telecom\n",
        "- Defines a schema for the dataset using PySpark's StructType and StructField.\n",
        "- Converts the simulated data into a PySpark DataFrame using the schema.\n",
        "\n",
        "- Data Display\n",
        "Displays the dataset using the show() method.\n",
        "\n",
        "- Feature Engineering\n",
        "Uses VectorAssembler to combine multiple feature columns (CallDuration, MessagesSent, DataUsage) into a single feature vector.\n",
        "Optionally scales the features using StandardScaler to normalize the data.\n",
        "\n",
        "- Pipeline Creation\n",
        "Creates a PySpark ML pipeline to automate data preprocessing, combining feature assembly and scaling steps.\n",
        "\n",
        "- Train-Test Split\n",
        "Splits the dataset into training and testing sets to ensure proper model evaluation.\n",
        "\n",
        "- Model Training\n",
        "Trains a regression model (likely LinearRegression or similar) using the training data.\n",
        "\n",
        "- Model Evaluation\n",
        "Evaluates the trained regression model on the test dataset.\n",
        "Calculates metrics such as RMSE (Root Mean Squared Error), RÂ², or others to assess performance.\n",
        "\n",
        "- Predictions\n",
        "Applies the trained regression model to the test data to predict target values (e.g., Revenue). Displays predictions and compares them to the true values.\n",
        "\n",
        "- Result Display\n",
        "Summarizes and visualizes model performance and predictions."
      ],
      "metadata": {
        "id": "wF4ESDErNgOR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Spark ML and Spark MLlib for Regression\n",
        "\n",
        "**Spark MLlib (Legacy):**\n",
        "\n",
        "* **Overview:** Spark MLlib was the original machine learning library in Apache Spark. While still functional, it's now largely superseded by Spark ML.  MLlib primarily uses RDDs (Resilient Distributed Datasets) for data representation.\n",
        "* **Regression Algorithms:** MLlib offered algorithms like Linear Regression, Logistic Regression, Support Vector Machines (SVM), and decision trees for regression tasks.  These algorithms were often implemented as lower-level APIs, requiring more manual configuration.\n",
        "* **Data Handling:**  Because it uses RDDs, data manipulation and preprocessing often involved a series of transformations and actions on the RDDs, which could sometimes be less intuitive or efficient compared to DataFrames.\n",
        "* **Limitations:** MLlib's API is generally considered less user-friendly and flexible than Spark ML's.  It's also less actively developed and supported.\n",
        "\n",
        "\n",
        "**Spark ML :**\n",
        "\n",
        "\n",
        "* **Overview:** Spark ML is the newer and preferred machine learning library in Spark. It uses DataFrames as its primary data structure, making it significantly more convenient and efficient for data manipulation and preprocessing. DataFrames offer schema, which enables better performance, especially for complex data structures.\n",
        "* **Pipeline API:** Spark ML introduces a powerful Pipeline API, which allows users to define sequences of data transformation and model training steps. This makes workflows more organized, repeatable, and easier to manage.  Pipelines promote modularity.\n",
        "* **Estimators and Transformers:** Spark ML represents algorithms as *Estimators* (fittable) and *Transformers* (transforming data). This clear distinction simplifies model building and the application of trained models to new data.  Estimators are fit to data producing a model; Transformers use fitted models to transform data.  This promotes a consistent API.\n",
        "* **Feature Engineering:**  Spark ML provides a rich set of tools for feature engineering, including feature scaling, one-hot encoding, and vector assemblers.  This is crucial for achieving optimal model performance. It is easier to assemble features with this approach.\n",
        "* **Regression Algorithms:**  Spark ML includes several algorithms suited for regression, such as Linear Regression, Generalized Linear Regression, Decision Tree Regression, Random Forest Regression, Gradient-Boosted Trees Regression (GBT Regression), and others.  These algorithms benefit from the DataFrame-based API.\n",
        "* **Model Evaluation:**  Spark ML provides evaluation metrics such as Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), R-squared, to assess model accuracy.\n",
        "* **Hyperparameter Tuning:** Using cross-validation and parameter grids, one can optimize the model's hyperparameters, which are parameters that are not learned from the data. Hyperparameter Tuning is essential to optimal model performance.\n",
        "\n",
        "\n",
        "**Regression in Spark ML :**\n",
        "\n",
        "1. **Data Preparation:** Load the data into a DataFrame, handle missing values, and perform feature engineering (e.g., one-hot encoding categorical variables, scaling numerical features).\n",
        "\n",
        "2. **Vector Assembler:** Combine features into a single vector column (required by many algorithms).\n",
        "\n",
        "3. **Model Selection:**  Choose an appropriate regression algorithm (Linear Regression, Decision Trees, Random Forests, GBT, etc.). The choice depends on data characteristics (linearity, non-linearity, number of features) and desired performance.\n",
        "\n",
        "4. **Pipeline Creation:** Create a pipeline that includes feature transformations (vector assembler, scalers, encoders) and the selected regression model. This ensures a reusable workflow.\n",
        "\n",
        "5. **Train-Test Split:** Split the data into training and testing sets.\n",
        "\n",
        "6. **Model Training:** Train the model on the training set using the pipeline.\n",
        "\n",
        "7. **Model Evaluation:** Use the trained model to predict on the testing set and evaluate its performance using appropriate metrics (RMSE, R-squared, MAE, etc.).\n",
        "\n",
        "8. **Hyperparameter Tuning:**  Use CrossValidator or TrainValidationSplit to fine-tune the model's hyperparameters for optimal performance.\n",
        "\n",
        "9. **Deployment:**  Save the trained model for deployment and later use.\n",
        "\n",
        "\n",
        "**Key Advantages of Spark ML:**\n",
        "\n",
        "* **Scalability:** Handles large datasets efficiently.\n",
        "* **Ease of Use:** DataFrame-based API makes data handling and preprocessing straightforward.\n",
        "* **Pipeline API:** Streamlines model development and deployment.\n",
        "* **Rich Feature Set:** Includes a wide variety of algorithms and feature engineering tools.\n",
        "\n",
        "Choosing between MLlib and ML: *Always* choose Spark ML due to its superior performance, features and ease of use. MLlib is deprecated.\n"
      ],
      "metadata": {
        "id": "h8vV5APbw5YB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook explores the use of Spark ML and Spark MLlib for regression tasks. It provides a comparison between the two libraries, highlighting the advantages of Spark ML and demonstrating a typical regression workflow using Spark ML."
      ],
      "metadata": {
        "id": "xxec0LDVxaob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Case Study - predicting customer revenue\n",
        "\n",
        "The objective of this case study is to build a basic regression model using Spark ML to predict customer revenue based on their usage metrics, such as call duration, number of messages sent, and data usage."
      ],
      "metadata": {
        "id": "rWxtcxOaWmzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is a simulated telecom dataset with the following features:\n",
        "\n",
        "UserID:\n",
        "A unique identifier for each customer.\n",
        "\n",
        "CallDuration (numeric):\n",
        "Total minutes of calls made by the customer in a specific period.\n",
        "\n",
        "MessagesSent (numeric):\n",
        "Total number of messages sent by the customer.\n",
        "\n",
        "DataUsage (numeric):\n",
        "Total data consumed by the customer (in GB).\n",
        "\n",
        "Revenue (numeric):\n",
        "The revenue generated by the customer (target variable)."
      ],
      "metadata": {
        "id": "ufcln9uUXQdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz\n",
        "\n",
        "!tar xzf spark-3.5.3-bin-hadoop3.tgz"
      ],
      "metadata": {
        "id": "uMpIhUjpLKpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dbf1eca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75cb8d09-9e37-46d2-d417-76a7ee7ab2ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.3-bin-hadoop3\""
      ],
      "metadata": {
        "id": "kdqeFlTbf0lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "print(pyspark.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvhA-XamuBpy",
        "outputId": "b438ebb9-c9ad-44f4-9ef8-14c00b46ae73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.5.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"TelecomML\").getOrCreate()"
      ],
      "metadata": {
        "id": "SyL8qbNvuDvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated Telecom Dataset\n",
        "# Changed data types to floats\n",
        "data = [\n",
        "    (1, 200.0, 30.0, 15.0, 0.5),  # UserID, CallDuration, MessagesSent, DataUsage, Revenue\n",
        "    (2, 100.0, 20.0, 10.0, 0.3),\n",
        "    (3, 300.0, 50.0, 20.0, 0.8),\n",
        "    (4, 150.0, 25.0, 12.0, 0.4),\n",
        "    (5, 250.0, 40.0, 18.0, 0.7),\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"UserID\", IntegerType(), True),\n",
        "    StructField(\"CallDuration\", FloatType(), True),\n",
        "    StructField(\"MessagesSent\", FloatType(), True),\n",
        "    StructField(\"DataUsage\", FloatType(), True),\n",
        "    StructField(\"Revenue\", FloatType(), True),\n",
        "])\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)"
      ],
      "metadata": {
        "id": "aoY41TI7xjf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show dataset\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ji2CcqjtxkSL",
        "outputId": "46abc35d-a34c-4e1a-bd5b-e65d73a1bae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+------------+------------+---------+-------+\n",
            "|UserID|CallDuration|MessagesSent|DataUsage|Revenue|\n",
            "+------+------------+------------+---------+-------+\n",
            "|     1|       200.0|        30.0|     15.0|    0.5|\n",
            "|     2|       100.0|        20.0|     10.0|    0.3|\n",
            "|     3|       300.0|        50.0|     20.0|    0.8|\n",
            "|     4|       150.0|        25.0|     12.0|    0.4|\n",
            "|     5|       250.0|        40.0|     18.0|    0.7|\n",
            "+------+------------+------------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Assemble features into a single vector\n",
        "feature_columns = [\"CallDuration\", \"MessagesSent\", \"DataUsage\"]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "\n",
        "# Split into training and testing data\n",
        "train_data, test_data = df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "oe2bcjCkuHAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "# Initialize Linear Regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Revenue\")"
      ],
      "metadata": {
        "id": "aGRfhWnFuddc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[assembler, lr])"
      ],
      "metadata": {
        "id": "74Ks0Ai8xroc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = pipeline.fit(train_data)"
      ],
      "metadata": {
        "id": "4qW-LwThxsYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "HuMkUOvVxtfF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show predictions\n",
        "predictions.select(\"UserID\", \"features\", \"Revenue\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xkJzRhDxtUy",
        "outputId": "007ac8b9-d84f-4ad6-920d-7ccb03de8b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+-------+------------------+\n",
            "|UserID|         features|Revenue|        prediction|\n",
            "+------+-----------------+-------+------------------+\n",
            "|     3|[300.0,50.0,20.0]|    0.8|0.8999999761581466|\n",
            "+------+-----------------+-------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Initialize evaluator\n",
        "evaluator = RegressionEvaluator(labelCol=\"Revenue\", predictionCol=\"prediction\", metricName=\"rmse\")"
      ],
      "metadata": {
        "id": "elaEMouzugdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate RMSE\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Square Error (RMSE): {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBo5ytuSukJQ",
        "outputId": "d4214a24-afa5-4cbe-db00-efa19d4d656c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Square Error (RMSE): 0.09999996423721769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Project on Regression using SparkML - predicting bandwidth allocation\n",
        "\n",
        "Problem Statement\n",
        "Title: Predicting 5G Bandwidth Allocation Based on Quality of Service Metrics\n",
        "In the rapidly evolving domain of 5G networks, efficient allocation of bandwidth to users is critical to maintaining optimal Quality of Service (QoS). The goal of this project is to develop a regression model using Spark ML to predict the bandwidth allocated to users based on various QoS metrics.\n",
        "\n",
        "The Allocated Bandwidth is influenced by factors such as:\n",
        "\n",
        "- Signal strength\n",
        "- Latency\n",
        "- Required bandwidth\n",
        "- Resource allocation percentage\n",
        "- Type of application (e.g., video calls, streaming, emergency services).\n",
        "\n",
        "By accurately predicting the allocated bandwidth, telecom operators can:\n",
        "\n",
        "- Optimize resource utilization\n",
        "- Ensure equitable and efficient bandwidth distribution\n",
        "- Enhance the overall user experience"
      ],
      "metadata": {
        "id": "3FUO_NPxXXzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"5G_Bandwidth_Allocation\").getOrCreate()\n",
        "\n",
        "# Load dataset (replace 'path_to_file' with actual file path in Colab)\n",
        "file_path = \"/content/Quality of Service 5G.csv\"\n",
        "spark_df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "spark_df.show()"
      ],
      "metadata": {
        "id": "d1E9YGr1WSYG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b4b9734-b61b-4058-d97b-022ab8701e1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------+-------------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "|     Timestamp|User_ID|   Application_Type|Signal_Strength|Latency|Required_Bandwidth|Allocated_Bandwidth|Resource_Allocation|\n",
            "+--------------+-------+-------------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "|9/3/2023 10:00| User_1|         Video_Call|        -75 dBm|  30 ms|           10 Mbps|            15 Mbps|                70%|\n",
            "|9/3/2023 10:00| User_2|         Voice_Call|        -80 dBm|  20 ms|          100 Kbps|           120 Kbps|                80%|\n",
            "|9/3/2023 10:00| User_3|          Streaming|        -85 dBm|  40 ms|            5 Mbps|             6 Mbps|                75%|\n",
            "|9/3/2023 10:00| User_4|  Emergency_Service|        -70 dBm|  10 ms|            1 Mbps|           1.5 Mbps|                90%|\n",
            "|9/3/2023 10:00| User_5|      Online_Gaming|        -78 dBm|  25 ms|            2 Mbps|             3 Mbps|                85%|\n",
            "|9/3/2023 10:00| User_6|Background_Download|        -90 dBm|  50 ms|          500 Kbps|           550 Kbps|                70%|\n",
            "|9/3/2023 10:00| User_7|       Web_Browsing|        -88 dBm|  30 ms|            1 Mbps|             1 Mbps|                60%|\n",
            "|9/3/2023 10:00| User_8|    IoT_Temperature|        -95 dBm| 100 ms|           10 Kbps|            15 Kbps|                50%|\n",
            "|9/3/2023 10:00| User_9|    Video_Streaming|        -82 dBm|  35 ms|            3 Mbps|           3.5 Mbps|                80%|\n",
            "|9/3/2023 10:00|User_10|      File_Download|        -75 dBm|  45 ms|            2 Mbps|             2 Mbps|                70%|\n",
            "|9/3/2023 10:00|User_11|         Video_Call|        -76 dBm|  32 ms|           12 Mbps|            14 Mbps|                70%|\n",
            "|9/3/2023 10:00|User_12|      Online_Gaming|        -79 dBm|  24 ms|          2.5 Mbps|           2.8 Mbps|                75%|\n",
            "|9/3/2023 10:00|User_13|       Web_Browsing|        -87 dBm|  28 ms|          1.2 Mbps|           1.2 Mbps|                65%|\n",
            "|9/3/2023 10:00|User_14|          VoIP_Call|        -81 dBm|  22 ms|          150 Kbps|           170 Kbps|                85%|\n",
            "|9/3/2023 10:00|User_15|         Video_Call|        -74 dBm|  29 ms|           10 Mbps|            12 Mbps|                60%|\n",
            "|9/3/2023 10:00|User_16|          Streaming|        -84 dBm|  38 ms|          4.5 Mbps|             5 Mbps|                70%|\n",
            "|9/3/2023 10:00|User_17|  Emergency_Service|        -69 dBm|   9 ms|          1.2 Mbps|           1.3 Mbps|                80%|\n",
            "|9/3/2023 10:00|User_18|Background_Download|        -91 dBm|  48 ms|          600 Kbps|           600 Kbps|                55%|\n",
            "|9/3/2023 10:00|User_19|    IoT_Temperature|        -96 dBm| 105 ms|            8 Kbps|             9 Kbps|                70%|\n",
            "|9/3/2023 10:00|User_20|    Video_Streaming|        -83 dBm|  34 ms|          3.2 Mbps|           3.5 Mbps|                75%|\n",
            "+--------------+-------+-------------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace, col\n",
        "\n",
        "# Remove unwanted strings from specific columns\n",
        "cleaned_df = spark_df \\\n",
        "    .withColumn(\"Signal_Strength\", regexp_replace(col(\"Signal_Strength\"), \" dBm\", \"\").cast(\"int\")) \\\n",
        "    .withColumn(\"Latency\", regexp_replace(col(\"Latency\"), \" ms\", \"\").cast(\"int\")) \\\n",
        "    .withColumn(\"Required_Bandwidth\", regexp_replace(col(\"Required_Bandwidth\"), \" (Kbps|Mbps)\", \"\").cast(\"float\")) \\\n",
        "    .withColumn(\"Allocated_Bandwidth\", regexp_replace(col(\"Allocated_Bandwidth\"), \" (Kbps|Mbps)\", \"\").cast(\"float\")) \\\n",
        "    .withColumn(\"Resource_Allocation\", regexp_replace(col(\"Resource_Allocation\"), \"%\", \"\").cast(\"int\"))\n",
        "\n",
        "# Show the cleaned DataFrame\n",
        "cleaned_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKnceVGNqKri",
        "outputId": "605687cb-5cb3-4f45-e182-2078259da93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-------+-------------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "|     Timestamp|User_ID|   Application_Type|Signal_Strength|Latency|Required_Bandwidth|Allocated_Bandwidth|Resource_Allocation|\n",
            "+--------------+-------+-------------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "|9/3/2023 10:00| User_1|         Video_Call|            -75|     30|              10.0|               15.0|                 70|\n",
            "|9/3/2023 10:00| User_2|         Voice_Call|            -80|     20|             100.0|              120.0|                 80|\n",
            "|9/3/2023 10:00| User_3|          Streaming|            -85|     40|               5.0|                6.0|                 75|\n",
            "|9/3/2023 10:00| User_4|  Emergency_Service|            -70|     10|               1.0|                1.5|                 90|\n",
            "|9/3/2023 10:00| User_5|      Online_Gaming|            -78|     25|               2.0|                3.0|                 85|\n",
            "|9/3/2023 10:00| User_6|Background_Download|            -90|     50|             500.0|              550.0|                 70|\n",
            "|9/3/2023 10:00| User_7|       Web_Browsing|            -88|     30|               1.0|                1.0|                 60|\n",
            "|9/3/2023 10:00| User_8|    IoT_Temperature|            -95|    100|              10.0|               15.0|                 50|\n",
            "|9/3/2023 10:00| User_9|    Video_Streaming|            -82|     35|               3.0|                3.5|                 80|\n",
            "|9/3/2023 10:00|User_10|      File_Download|            -75|     45|               2.0|                2.0|                 70|\n",
            "|9/3/2023 10:00|User_11|         Video_Call|            -76|     32|              12.0|               14.0|                 70|\n",
            "|9/3/2023 10:00|User_12|      Online_Gaming|            -79|     24|               2.5|                2.8|                 75|\n",
            "|9/3/2023 10:00|User_13|       Web_Browsing|            -87|     28|               1.2|                1.2|                 65|\n",
            "|9/3/2023 10:00|User_14|          VoIP_Call|            -81|     22|             150.0|              170.0|                 85|\n",
            "|9/3/2023 10:00|User_15|         Video_Call|            -74|     29|              10.0|               12.0|                 60|\n",
            "|9/3/2023 10:00|User_16|          Streaming|            -84|     38|               4.5|                5.0|                 70|\n",
            "|9/3/2023 10:00|User_17|  Emergency_Service|            -69|      9|               1.2|                1.3|                 80|\n",
            "|9/3/2023 10:00|User_18|Background_Download|            -91|     48|             600.0|              600.0|                 55|\n",
            "|9/3/2023 10:00|User_19|    IoT_Temperature|            -96|    105|               8.0|                9.0|                 70|\n",
            "|9/3/2023 10:00|User_20|    Video_Streaming|            -83|     34|               3.2|                3.5|                 75|\n",
            "+--------------+-------+-------------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of rows and columns\n",
        "print(f\"Total Rows: {spark_df.count()}\")\n",
        "print(f\"Total Columns: {len(spark_df.columns)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nquaJunIn5bJ",
        "outputId": "85925063-b33d-4bfe-fe3e-e17d89c4b5bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows: 400\n",
            "Total Columns: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col,isnan, when, count\n",
        "\n",
        "cleaned_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in cleaned_df.columns]).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPnYr0J5nqPf",
        "outputId": "d99fe478-1333-4496-8f8e-a45de43ef13d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+----------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "|Timestamp|User_ID|Application_Type|Signal_Strength|Latency|Required_Bandwidth|Allocated_Bandwidth|Resource_Allocation|\n",
            "+---------+-------+----------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "|        0|      0|               0|              0|      0|                 0|                  0|                  0|\n",
            "+---------+-------+----------------+---------------+-------+------------------+-------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "# Encode categorical column\n",
        "indexer = StringIndexer(inputCol=\"Application_Type\", outputCol=\"Application_Index\")"
      ],
      "metadata": {
        "id": "IetYiRQXd4h8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble features into a single vector\n",
        "feature_columns = [\"Signal_Strength\", \"Latency\", \"Required_Bandwidth\", \"Resource_Allocation\", \"Application_Index\"]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
      ],
      "metadata": {
        "id": "TxWeCrGnfsMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# Initialize Linear Regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Allocated_Bandwidth\")"
      ],
      "metadata": {
        "id": "QKOslddTeEW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[indexer, assembler, lr])"
      ],
      "metadata": {
        "id": "qb-kd43TfOqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "train_data, test_data = cleaned_df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "h8KjrGoyfmvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = pipeline.fit(train_data)"
      ],
      "metadata": {
        "id": "U2RAMcbxfofa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(test_data)\n",
        "\n",
        "# Show predictions\n",
        "predictions.select(\"features\", \"Allocated_Bandwidth\", \"prediction\").show()"
      ],
      "metadata": {
        "id": "vHygQeU8gGrL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9072b4f-b491-4145-e046-a6c8e81b4ad9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+-------------------+\n",
            "|            features|Allocated_Bandwidth|         prediction|\n",
            "+--------------------+-------------------+-------------------+\n",
            "|[-76.0,32.0,12.0,...|               14.0| 13.686394859125464|\n",
            "|[-74.0,29.0,10.0,...|               12.0|   5.19777683669836|\n",
            "|[-69.0,9.0,1.2000...|                1.3|   5.56509596082433|\n",
            "|[-77.0,31.0,11.0,...|               13.0| 15.912271788979972|\n",
            "|[-68.0,8.0,1.1000...|                1.2|-0.9547421177030131|\n",
            "|[-84.0,33.0,3.400...|                3.7|0.27815617734280096|\n",
            "|[-82.0,36.0,4.0,8...|                4.6| 11.171584753116925|\n",
            "|[-79.0,29.0,10.5,...|               12.3|  12.32637927087034|\n",
            "|[-86.0,31.0,3.599...|                3.9| 0.5951779176391199|\n",
            "|[-90.0,50.0,500.0...|              550.0|  505.9951888442168|\n",
            "|[-88.0,30.0,1.0,6...|                1.0|-4.9864736569116275|\n",
            "|[-82.0,35.0,3.0,8...|                3.5|  6.135364392929517|\n",
            "|[-86.0,18.0,155.0...|              180.0| 172.03014393714906|\n",
            "|[-77.0,32.0,4.5,8...|                5.1| 11.474617546400275|\n",
            "|[-77.0,24.0,2.599...|                2.9| 11.276121325765466|\n",
            "|[-100.0,103.0,4.0...|                5.0| 2.5873784232570785|\n",
            "|[-64.0,7.0,1.0,70...|                1.1|-1.2407287804719331|\n",
            "|[-90.0,29.0,4.0,7...|                4.3|  1.212368891393588|\n",
            "|[-94.0,24.0,0.699...|                0.7|  1.424997970008107|\n",
            "|[-61.0,6.0,0.6999...|                0.8|-1.6825557086404004|\n",
            "+--------------------+-------------------+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator"
      ],
      "metadata": {
        "id": "RZg79_c6gNvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predictions\n",
        "evaluator = RegressionEvaluator(labelCol=\"Allocated_Bandwidth\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "Xmda-DgDgR6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate R-squared\n",
        "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R2): {r2}\")"
      ],
      "metadata": {
        "id": "pYJbPd6IgULx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bac00b05-4626-41e1-cf88-a9677976664d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 7.159055459189656\n",
            "R-squared (R2): 0.9989080949744653\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice Case Study - bandwidth prediction on custom dataset"
      ],
      "metadata": {
        "id": "klxs_tzUj64G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# data = [\n",
        "    (1, -70.0, 20.0, 10.0, 5.0, \"Video Call\", 5.0),\n",
        "    (2, -65.0, 15.0, 5.0, 3.0, \"Streaming\", 3.5),\n",
        "    (3, -75.0, 25.0, 20.0, 10.0, \"Emergency\", 18.0),\n",
        "    (4, -80.0, 30.0, 10.0, 5.0, \"Streaming\", 6.0),\n",
        "    (5, -60.0, 10.0, 2.0, 2.0, \"Video Call\", 2.0),\n",
        "    (6, -72.0, 22.0, 12.0, 6.0, \"Video Call\", 7.0),\n",
        "    (7, -68.0, 18.0, 8.0, 4.0, \"Streaming\", 4.0),\n",
        "    (8, -78.0, 28.0, 15.0, 7.0, \"Emergency\", 15.0),\n",
        "    (9, -63.0, 12.0, 3.0, 1.0, \"Video Call\", 1.5),\n",
        "    (10, -73.0, 23.0, 13.0, 6.5, \"Streaming\", 8.0),\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "S1hLObACjAYI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To create a regression model using Spark ML with this dataset, participants are supposed to perform the following steps:\n",
        "\n",
        "1. **Data Loading and Preparation:** Load the dataset into a Spark DataFrame.  Ensure data types are correct (numeric for features and the target variable). Handle missing values appropriately (imputation or removal).  Perform any necessary data cleaning, like removing units from numerical columns if they exist, and converting them to the appropriate numerical data type.\n",
        "\n",
        "\n",
        "2. **Feature Engineering:** If needed, create new features or transform existing ones. For instance, you might create interaction terms between features or apply transformations to better represent the data. Encode any categorical variables (e.g., application type in the 5G example) using methods like StringIndexer, creating numerical representations for them.\n",
        "\n",
        "\n",
        "3. **Vector Assembler:**  Combine the relevant features into a single vector column. This is a crucial step as many ML algorithms in Spark expect a single vector column of input features.\n",
        "\n",
        "\n",
        "4. **Data Splitting:** Divide the dataset into training and testing sets.  This is standard practice to assess how well your model generalizes to unseen data.\n",
        "\n",
        "\n",
        "5. **Model Selection:** Choose a suitable regression algorithm.  Linear Regression is a good starting point, especially if you believe there's a linear relationship between the features and the target variable.  Other options include Decision Tree Regression, Random Forest Regression, and Gradient-Boosted Trees (GBT) for more complex relationships.\n",
        "\n",
        "\n",
        "6. **Pipeline Creation (Recommended):** Build a pipeline that includes feature transformations and the chosen regression model. Pipelines make the entire workflow manageable, reproducible, and easier to tune.\n",
        "\n",
        "\n",
        "7. **Model Training:** Train the model on the training data using the pipeline.\n",
        "\n",
        "\n",
        "8. **Model Evaluation:** Evaluate the model's performance on the testing data. Common evaluation metrics for regression are RMSE, MAE, and R-squared. Use a RegressionEvaluator to calculate these metrics.\n",
        "\n",
        "\n",
        "9. **Hyperparameter Tuning (Important):** Fine-tune the model's hyperparameters (parameters that aren't learned from the data itself).  Techniques like cross-validation help optimize these parameters for better performance."
      ],
      "metadata": {
        "id": "I2L_ZJCKk-RI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, FloatType, IntegerType, StringType # Import StringType\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.feature import StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.sql.types import FloatType\n",
        "from pyspark.sql.functions import regexp_replace, col\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler"
      ],
      "metadata": {
        "id": "7Sh-4g4JhzLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"5G_Bandwidth_Allocation\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, -70.0, 20.0, 10.0, 5.0, \"Video Call\", 5.0),\n",
        "    (2, -65.0, 15.0, 5.0, 3.0, \"Streaming\", 3.5),\n",
        "    (3, -75.0, 25.0, 20.0, 10.0, \"Emergency\", 18.0),\n",
        "    (4, -80.0, 30.0, 10.0, 5.0, \"Streaming\", 6.0),\n",
        "    (5, -60.0, 10.0, 2.0, 2.0, \"Video Call\", 2.0),\n",
        "    (6, -72.0, 22.0, 12.0, 6.0, \"Video Call\", 7.0),\n",
        "    (7, -68.0, 18.0, 8.0, 4.0, \"Streaming\", 4.0),\n",
        "    (8, -78.0, 28.0, 15.0, 7.0, \"Emergency\", 15.0),\n",
        "    (9, -63.0, 12.0, 3.0, 1.0, \"Video Call\", 1.5),\n",
        "    (10, -73.0, 23.0, 13.0, 6.5, \"Streaming\", 8.0),\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"UserID\", IntegerType(), True),\n",
        "    StructField(\"Signal_Strength\", FloatType(), True),\n",
        "    StructField(\"Latency\", FloatType(), True),\n",
        "    StructField(\"Required_Bandwidth\", FloatType(), True),\n",
        "    StructField(\"Resource_Allocation\", FloatType(), True),\n",
        "    StructField(\"Application_Type\", StringType(), True),\n",
        "    StructField(\"Allocated_Bandwidth\", FloatType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "-sPdzaWBRyfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark_df = spark.createDataFrame(data, schema=schema)"
      ],
      "metadata": {
        "id": "jYcva4VhRzjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical column\n",
        "indexer = StringIndexer(inputCol=\"Application_Type\", outputCol=\"Application_Index\")"
      ],
      "metadata": {
        "id": "Olxq7Ed6R27D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assemble features into a single vector\n",
        "feature_columns = [\"Signal_Strength\", \"Latency\", \"Required_Bandwidth\", \"Resource_Allocation\", \"Application_Index\"]\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")"
      ],
      "metadata": {
        "id": "6FKd89S7R23o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Linear Regression model\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Allocated_Bandwidth\")"
      ],
      "metadata": {
        "id": "tOl_w8NbR21L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline\n",
        "pipeline = Pipeline(stages=[indexer, assembler, lr])"
      ],
      "metadata": {
        "id": "1Q1EE9K6R2yv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "train_data, test_data = spark_df.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "Ebi9ZW21SFXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "model = pipeline.fit(train_data)"
      ],
      "metadata": {
        "id": "O80JNryvSHlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "predictions = model.transform(test_data)"
      ],
      "metadata": {
        "id": "qjFSWH8lSJ-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show predictions\n",
        "predictions.select(\"features\", \"Allocated_Bandwidth\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cYytnMjmSOOE",
        "outputId": "2bb7c2cb-867f-4580-8b28-e63834b8f467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+------------------+\n",
            "|            features|Allocated_Bandwidth|        prediction|\n",
            "+--------------------+-------------------+------------------+\n",
            "|[-75.0,25.0,20.0,...|               18.0|17.352974344514656|\n",
            "|[-72.0,22.0,12.0,...|                7.0|  9.25509820505565|\n",
            "+--------------------+-------------------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate predictions\n",
        "evaluator = RegressionEvaluator(labelCol=\"Allocated_Bandwidth\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)"
      ],
      "metadata": {
        "id": "VWAMZX4MSQnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate R-squared\n",
        "r2 = evaluator.evaluate(predictions, {evaluator.metricName: \"r2\"})\n",
        "\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n",
        "print(f\"R-squared (R2): {r2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8qSHGU2SSuU",
        "outputId": "295768b5-ee57-49f5-f7dd-5bd2dcf1ee6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE): 1.658931902354863\n",
            "R-squared (R2): 0.9090229733338604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8AFyH0-Sf_3L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}