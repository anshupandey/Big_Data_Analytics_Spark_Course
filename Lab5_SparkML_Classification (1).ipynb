{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Lab 5 : Spark ML for Classification"
      ],
      "metadata": {
        "id": "C4KoOeoaJtGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Tasks\n",
        "\n",
        "- Installation and Setup\n",
        "Installs the PySpark library using pip install pyspark.\n",
        "Imports necessary modules from PySpark, including SparkSession.\n",
        "\n",
        "- Spark Session Initialization\n",
        "Initializes a Spark sessions.\n",
        "\n",
        "- Dataset Preparation\n",
        "Simulates a telecom fraud detection dataset with fields such as:\n",
        "TransactionID (integer)\n",
        "TransactionType (string)\n",
        "CallDuration (float)\n",
        "TransactionFrequency (integer)\n",
        "DeviationScore (integer)\n",
        "Fraudulent (integer, binary: 0 or 1).\n",
        "Defines a schema for the dataset using StructType and StructField.\n",
        "Creates a PySpark DataFrame using the simulated data and defined schema.\n",
        "\n",
        "- Data Display\n",
        "Uses df.show() to display the initial dataset.\n",
        "\n",
        "- Data Preprocessing\n",
        "Imports modules for data transformation: StringIndexer, VectorAssembler, and Pipeline.\n",
        "Converts the categorical column TransactionType into a numerical index using StringIndexer.\n",
        "Combines multiple feature columns (TypeIndex, CallDuration, TransactionFrequency, DeviationScore) into a single feature vector using VectorAssembler.\n",
        "\n",
        "- Pipeline Creation\n",
        "Creates a PySpark ML pipeline for data preprocessing, combining transformations like string indexing and feature vector assembly.\n",
        "\n",
        "- Train-Test Split\n",
        "Splits the data into training and testing sets using the randomSplit function.\n",
        "\n",
        "- Model Training\n",
        "Uses a classification algorithm (likely LogisticRegression or similar) to train a machine learning model on the preprocessed training data.\n",
        "\n",
        "- Model Evaluation\n",
        "Evaluates the trained model on the test dataset.\n",
        "Calculates performance metrics such as accuracy or other evaluation measures using the MulticlassClassificationEvaluator.\n",
        "\n",
        "- Predictions\n",
        "Applies the trained model to the test dataset to generate predictions.\n",
        "Displays predictions alongside true labels.\n",
        "\n",
        "- Result Display\n",
        "Shows the results of predictions and evaluates the model's performance."
      ],
      "metadata": {
        "id": "XyOlKImKJ1iB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Introduction:**\n",
        "\n",
        "Apache Spark's MLlib library provides a powerful and scalable platform for machine learning tasks, including classification. PySpark, the Python API for Spark, allows users to leverage this functionality with the familiar Python syntax.  This is particularly valuable for handling large datasets that wouldn't fit into the memory of a single machine.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "* **Resilient Distributed Datasets (RDDs):**  Spark operates on RDDs, which are fault-tolerant collections of elements distributed across a cluster.  MLlib algorithms are designed to work efficiently with RDDs, enabling parallel processing of vast amounts of data.  While DataFrames and Datasets are now preferred for their schema enforcement and optimization, understanding the underlying RDD concept is beneficial.\n",
        "* **DataFrames and Datasets:**  These are higher-level abstractions built on top of RDDs. They offer improved performance, schema enforcement, and optimization opportunities.  DataFrames and Datasets are the recommended way to work with structured data in Spark ML.\n",
        "* **Pipelines:** MLlib provides pipelines for chaining multiple stages of data transformation and model building. This simplifies the workflow and improves reproducibility.  Pipelines enable modularization and reusability of different stages of a machine learning pipeline.\n",
        "* **Estimators and Transformers:**  MLlib algorithms are implemented as estimators or transformers.  Estimators learn from data to create a model (e.g., Logistic Regression, Decision Tree).  Transformers take data and a model to produce transformed data (e.g., feature scaling, model prediction).\n",
        "* **Feature Engineering:**  Feature engineering plays a crucial role in the success of any machine learning model. PySpark provides tools for feature extraction, transformation, and selection, including methods for handling categorical features (one-hot encoding, string indexer), numerical features (scaling, standardization), and feature importance calculation.\n",
        "* **Model Evaluation:**  Evaluating model performance is essential. PySpark offers various metrics like accuracy, precision, recall, F1-score, AUC-ROC, and others for classification tasks.  Understanding which metric is most relevant for your problem is critical.\n",
        "* **Hyperparameter Tuning:**  Finding the optimal hyperparameters for your chosen model is crucial.  Grid search, random search, and cross-validation methods are commonly used within the Spark MLlib framework for efficient exploration of the hyperparameter space.\n",
        "* **Model Persistence:**  Trained models can be saved to disk and loaded back later, eliminating the need to retrain every time.  This enables model reuse and deployment in production environments.\n",
        "\n",
        "\n",
        "**Classification Algorithms in PySpark MLlib:**\n",
        "\n",
        "Several common classification algorithms are available in PySpark's MLlib:\n",
        "\n",
        "* **Logistic Regression:**  A linear model that predicts the probability of a categorical outcome.  It's widely used due to its simplicity and interpretability.\n",
        "* **Decision Trees:** Tree-based models that partition the data based on features to create a hierarchy of decisions leading to a predicted class.  They are relatively easy to interpret but can be prone to overfitting.\n",
        "* **Random Forests:** An ensemble of decision trees that improve predictive accuracy and reduce overfitting. They are highly versatile and robust.\n",
        "* **Gradient-Boosted Trees (GBTs):** Another ensemble method that sequentially builds trees, where each tree corrects the errors of the previous ones.  GBDTs are often among the top-performing classification algorithms.\n",
        "* **Support Vector Machines (SVMs):**  Finds the optimal hyperplane that separates classes in the feature space. They work well in high-dimensional spaces but can be computationally expensive.\n",
        "* **Naive Bayes:**  A probabilistic classifier based on Bayes' theorem with the assumption of feature independence.  It's simple and efficient, particularly suitable for text classification.\n",
        "* **Multilayer Perceptron (MLP):**  A neural network architecture that can model complex non-linear relationships.  It can be very powerful but requires careful tuning and often substantial computational resources.\n",
        "\n",
        "\n",
        "**Workflow:**\n",
        "\n",
        "A typical workflow in PySpark for classification involves:\n",
        "\n",
        "1. **Data Loading and Preparation:** Loading the data into a Spark DataFrame or Dataset.\n",
        "2. **Data Cleaning and Preprocessing:**  Handling missing values, outliers, and converting data types as needed.\n",
        "3. **Feature Engineering:** Creating or transforming features to improve model performance.\n",
        "4. **Feature Selection:**  Choosing relevant features to include in the model.\n",
        "5. **Splitting Data:** Partitioning the data into training and testing sets.\n",
        "6. **Model Training:** Selecting and training a classification model.\n",
        "7. **Hyperparameter Tuning:** Optimizing the model's hyperparameters.\n",
        "8. **Model Evaluation:** Evaluating the model's performance using metrics relevant to the task and finally deployment\n",
        "\n",
        "Understanding these concepts and tools will enable you to effectively use PySpark for building and deploying robust classification models for large datasets.\n",
        "\n",
        "Let us look at a scenario where we are building a ML classification model using Spark ML for Fraud Detection in Telecom Transactions.\n"
      ],
      "metadata": {
        "id": "Od2YnrteopIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Case Study - fraud detection in telecom transactions\n",
        "\n",
        "This scenario focuses on classifying telecom transactions as either fraudulent or non-fraudulent based on certain features, such as call duration, location, frequency of transactions, and whether the activity deviates from the user's typical patterns.\n",
        "\n",
        "The dataset contains:\n",
        "\n",
        "- TransactionType: Indicates whether the transaction is domestic or international.\n",
        "\n",
        "- CallDuration: Duration of the call in minutes.\n",
        "\n",
        "- TransactionFrequency: Number of transactions in the past week.\n",
        "\n",
        "- DeviationScore: A computed score indicating how much the transaction deviates from typical behavior.\n",
        "\n",
        "- Fraudulent: The target label (1 for fraudulent, 0 for non-fraudulent)."
      ],
      "metadata": {
        "id": "oZoFGKnyo3rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install and Configure PySpark\n",
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh1CRxNyoto-",
        "outputId": "6d02c5fa-34c3-4704-8c22-f00a831aecc6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Start Spark session\n",
        "spark = SparkSession.builder.appName(\"TelecomFraudDetection\").getOrCreate()"
      ],
      "metadata": {
        "id": "MjqMZEyNpvRZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and Simulate Telecom Fraud Dataset\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType"
      ],
      "metadata": {
        "id": "UsuF9zmAsRK-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Simulate a fraud detection dataset\n",
        "data = [\n",
        "    (1, \"domestic\", 5.5, 10, 3, 0),\n",
        "    (2, \"international\", 15.2, 50, 20, 1),\n",
        "    (3, \"domestic\", 3.1, 5, 2, 0),\n",
        "    (4, \"domestic\", 25.4, 100, 50, 1),\n",
        "    (5, \"international\", 12.3, 30, 10, 0),\n",
        "    (6, \"domestic\", 2.0, 1, 1, 0),\n",
        "    (7, \"international\", 50.5, 200, 100, 1),\n",
        "]\n",
        "schema = StructType([\n",
        "    StructField(\"TransactionID\", IntegerType(), True),\n",
        "    StructField(\"TransactionType\", StringType(), True),\n",
        "    StructField(\"CallDuration\", FloatType(), True),\n",
        "    StructField(\"TransactionFrequency\", IntegerType(), True),\n",
        "    StructField(\"DeviationScore\", IntegerType(), True),\n",
        "    StructField(\"Fraudulent\", IntegerType(), True),\n",
        "])\n",
        "df = spark.createDataFrame(data, schema=schema)"
      ],
      "metadata": {
        "id": "y8BvBp_WsZS9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Display data\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58Y3guhosrvm",
        "outputId": "ce1b7b51-63a7-48ae-f363-6bf4685d48bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+---------------+------------+--------------------+--------------+----------+\n",
            "|TransactionID|TransactionType|CallDuration|TransactionFrequency|DeviationScore|Fraudulent|\n",
            "+-------------+---------------+------------+--------------------+--------------+----------+\n",
            "|            1|       domestic|         5.5|                  10|             3|         0|\n",
            "|            2|  international|        15.2|                  50|            20|         1|\n",
            "|            3|       domestic|         3.1|                   5|             2|         0|\n",
            "|            4|       domestic|        25.4|                 100|            50|         1|\n",
            "|            5|  international|        12.3|                  30|            10|         0|\n",
            "|            6|       domestic|         2.0|                   1|             1|         0|\n",
            "|            7|  international|        50.5|                 200|           100|         1|\n",
            "+-------------+---------------+------------+--------------------+--------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml import Pipeline"
      ],
      "metadata": {
        "id": "XfegNa2ws4uy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert categorical columns to numerical\n",
        "type_indexer = StringIndexer(inputCol=\"TransactionType\", outputCol=\"TypeIndex\")\n",
        "\n",
        "#Combine features into a single vector\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"TypeIndex\", \"CallDuration\", \"TransactionFrequency\", \"DeviationScore\"],\n",
        "    outputCol=\"features\"\n",
        ")"
      ],
      "metadata": {
        "id": "P2p82KJ1s6go"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a pipeline for preprocessing\n",
        "pipeline = Pipeline(stages=[type_indexer, assembler])\n",
        "preprocessed_data = pipeline.fit(df).transform(df)\n",
        "\n",
        "#Display preprocessed data\n",
        "preprocessed_data.select(\"features\", \"Fraudulent\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pF-nV9Hls6da",
        "outputId": "fd103097-1aa9-4bf5-f947-85825668d337"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+\n",
            "|            features|Fraudulent|\n",
            "+--------------------+----------+\n",
            "|  [0.0,5.5,10.0,3.0]|         0|\n",
            "|[1.0,15.199999809...|         1|\n",
            "|[0.0,3.0999999046...|         0|\n",
            "|[0.0,25.399999618...|         1|\n",
            "|[1.0,12.300000190...|         0|\n",
            "|   [0.0,2.0,1.0,1.0]|         0|\n",
            "|[1.0,50.5,200.0,1...|         1|\n",
            "+--------------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train a Classification Model\n",
        "from pyspark.ml.classification import RandomForestClassifier"
      ],
      "metadata": {
        "id": "oCFhp7IWs6Wu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split data into training and test sets\n",
        "train, test = preprocessed_data.randomSplit([0.8, 0.2], seed=123)"
      ],
      "metadata": {
        "id": "2dhsu_n9tQFx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize and train the Random Forest classifier\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"Fraudulent\")\n",
        "model = rf.fit(train)"
      ],
      "metadata": {
        "id": "TMt8TzMntaig"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the Model\n",
        "predictions = model.transform(test)\n",
        "predictions.select(\"features\", \"Fraudulent\", \"prediction\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4vKXScz3tae-",
        "outputId": "050728be-4b6d-452f-f83f-c0446f5d8ffb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+----------+\n",
            "|            features|Fraudulent|prediction|\n",
            "+--------------------+----------+----------+\n",
            "|[0.0,3.0999999046...|         0|       0.0|\n",
            "|   [0.0,2.0,1.0,1.0]|         0|       0.0|\n",
            "+--------------------+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate using MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"Fraudulent\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpkSZqzjtacW",
        "outputId": "0ca1a747-9319-4921-aee6-5e4e69d9a6dc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project on Classification using Spark ML - Network  Slicing Recognition"
      ],
      "metadata": {
        "id": "Z4Q56BY2FON0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to build a machine learning classification model to predict the type of network slice (slice Type) based on the provided features. Accurate classification helps in:\n",
        "\n",
        "- Automating network resource allocation.\n",
        "- Enhancing the efficiency of service-specific configurations.\n",
        "- Supporting real-time decision-making for optimizing LTE/5G network performance.\n",
        "\n",
        "#Steps to Build the Classification Model\n",
        "\n",
        "- Data Loading and Inspection:\n",
        "\n",
        "- Load the dataset into a PySpark DataFrame and inspect for any anomalies or issues in the data.\n",
        "\n",
        "- Data Preprocessing:\n",
        "Column Renaming: Resolve naming issues (e.g., spaces in column names like \"Industry 4.0\").\n",
        "Handle Missing Values: Impute or remove rows with null or invalid data.\n",
        "Feature Assembly: Combine all relevant features into a single vector using VectorAssembler.\n",
        "Label Encoding: Convert the target variable (slice Type) into numerical labels using StringIndexer.\n",
        "\n",
        "- Train-Test Split:\n",
        "Partition the dataset into training (80%) and testing (20%) subsets to evaluate model performance.\n",
        "\n",
        "- Model Training:\n",
        "Train a classification model using Random Forest, which is robust and handles numerical and categorical features well.\n",
        "\n",
        "- Model Evaluation:\n",
        "Use metrics like accuracy to evaluate the modelâ€™s performance on the test set using MulticlassClassificationEvaluator."
      ],
      "metadata": {
        "id": "N5FCTty7I6qY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required PySpark libraries\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "metadata": {
        "id": "gnqRhc2WFe0v"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Spark session\n",
        "spark = SparkSession.builder.appName(\"NetworkSlicingClassification\").getOrCreate()"
      ],
      "metadata": {
        "id": "zAntuSn0Fjur"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"/content/network_slicing_recognition.csv\"\n",
        "data = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "data.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RG4BNmgZFjrE",
        "outputId": "bf6be968-6264-4238-90d0-08a45033917f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+----+----------------+------------+---+------+---+-------+------------+----------+------------+-----------+-------------+-----------------+--------------------+----------+----------+\n",
            "|LTE/5g Category|Time|Packet Loss Rate|Packet delay|IoT|LTE/5G|GBR|Non-GBR|AR/VR/Gaming|Healthcare|Industry 4.0|IoT Devices|Public Safety|Smart City & Home|Smart Transportation|Smartphone|slice Type|\n",
            "+---------------+----+----------------+------------+---+------+---+-------+------------+----------+------------+-----------+-------------+-----------------+--------------------+----------+----------+\n",
            "|             14|   0|          1.0E-6|          10|  1|     0|  0|      1|           0|         0|           0|          0|            1|                0|                   0|         0|         3|\n",
            "|             18|  20|           0.001|         100|  0|     1|  1|      0|           1|         0|           0|          0|            0|                0|                   0|         0|         1|\n",
            "|             17|  14|          1.0E-6|         300|  0|     1|  0|      1|           0|         0|           0|          0|            0|                0|                   0|         1|         1|\n",
            "|              3|  17|            0.01|         100|  0|     1|  0|      1|           0|         0|           0|          0|            0|                0|                   0|         1|         1|\n",
            "|              9|   4|            0.01|          50|  1|     0|  0|      1|           0|         0|           0|          0|            0|                1|                   0|         0|         2|\n",
            "+---------------+----+----------------+------------+---+------+---+-------+------------+----------+------------+-----------+-------------+-----------------+--------------------+----------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing\n",
        "# Rename problematic columns to avoid errors\n",
        "data = data.withColumnRenamed(\"Industry 4.0\", \"Industry_4_0\")\n",
        "\n",
        "# Assemble features into a single vector\n",
        "feature_columns = [\n",
        "    \"Time\", \"Packet Loss Rate\", \"Packet delay\", \"IoT\", \"LTE/5G\", \"GBR\", \"Non-GBR\",\n",
        "    \"AR/VR/Gaming\", \"Healthcare\", \"Industry_4_0\", \"IoT Devices\", \"Public Safety\",\n",
        "    \"Smart City & Home\", \"Smart Transportation\", \"Smartphone\"\n",
        "]\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
        "data = assembler.transform(data)\n"
      ],
      "metadata": {
        "id": "HEnx5eH7Fjop"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert target variable to indexed labels\n",
        "label_indexer = StringIndexer(inputCol=\"slice Type\", outputCol=\"label\")\n",
        "data = label_indexer.fit(data).transform(data)"
      ],
      "metadata": {
        "id": "kPq_0R8VFrsD"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and testing sets\n",
        "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
      ],
      "metadata": {
        "id": "rPulWmN6GEJQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=20, maxDepth=5)\n",
        "model = rf.fit(train_data)"
      ],
      "metadata": {
        "id": "53qoZYK1GEF2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions\n",
        "test_predictions = model.transform(test_data)\n",
        "test_predictions.select(\"features\", \"label\", \"prediction\").show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Y53boxTIYWp",
        "outputId": "fc4da52d-8875-4a51-869a-c21f2d5c6a85"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+\n",
            "|            features|label|prediction|\n",
            "+--------------------+-----+----------+\n",
            "|(15,[1,2,3,6,13],...|  2.0|       2.0|\n",
            "|(15,[1,2,3,6,13],...|  2.0|       2.0|\n",
            "|(15,[1,2,3,6,11],...|  2.0|       2.0|\n",
            "|(15,[1,2,3,6,8],[...|  2.0|       2.0|\n",
            "|(15,[1,2,4,5,14],...|  0.0|       0.0|\n",
            "+--------------------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(test_predictions)\n",
        "print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1VoA-xMIYTQ",
        "outputId": "c2183f73-e4f5-44c5-d518-0457a9dcd243"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "_50NW8RoIYQy"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice Case Study: customer churn prediction in telecom\n",
        "\n",
        "This case study focuses on predicting customer churn in a telecom company. Customer churn, the rate at which customers stop doing business with a company, is a critical metric for businesses. Predicting which customers are likely to churn allows companies to implement retention strategies and reduce customer loss.\n",
        "\n",
        "The dataset used in this analysis includes various customer features potentially related to churn. These features can be broadly categorized into:\n",
        "\n",
        "- Demographics: Customer age, gender\n",
        "- Account Information: Tenure (how long they've been a customer), contract type, monthly charges, total charges\n",
        "- Service Usage: Number of phone lines, multiple lines, internet service, online security, tech support, streaming services, etc.\n",
        "- Payment Information: Payment method\n",
        "\n",
        "\n",
        "Workflow:\n",
        "\n",
        "- Data Loading and Preparation: Load the simulated data into a Spark DataFrame.\n",
        "- Data Cleaning and Preprocessing: Handle missing values (if any) and convert categorical features to numerical representations using StringIndexer.\n",
        "- Feature Scaling: Standardize numerical features using StandardScaler.\n",
        "- Feature Engineering: Create a feature vector using VectorAssembler.\n",
        "- Model Selection and Training:** Train multiple classification models (Logistic Regression, Random Forest, Gradient-Boosted Trees) to predict churn.\n",
        "- Hyperparameter Tuning: Use CrossValidator to optimize model hyperparameters.\n",
        "- Model Evaluation: Evaluate the performance of the models using suitable metrics (accuracy, precision, recall, F1-score, AUC-ROC).\n",
        "- Model Selection: Choose the best performing model based on the evaluation."
      ],
      "metadata": {
        "id": "d15Q0ofq2OtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# data = [\n",
        "    (1, \"Male\", 34, 2, \"Month-to-month\", 65.5, 1300.0, 0),\n",
        "    (2, \"Female\", 45, 12, \"One year\", 85.2, 10224.0, 0),\n",
        "    (3, \"Male\", 22, 1, \"Month-to-month\", 55.1, 551.0, 1),\n",
        "    (4, \"Female\", 58, 7, \"Two year\", 95.4, 6678.0, 0),\n",
        "    (5, \"Male\", 30, 3, \"Month-to-month\", 70.3, 2109.0, 1)\n",
        "]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EXKAdMx72KFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DoubleType\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
        "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
      ],
      "metadata": {
        "id": "bow93tht0pDo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Spark Configuration:\n",
        "spark = SparkSession.builder.appName(\"TelecomChurnPrediction\").getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, \"Male\", 34, 2, \"Month-to-month\", 65.5, 1300.0, 0),\n",
        "    (2, \"Female\", 45, 12, \"One year\", 85.2, 10224.0, 0),\n",
        "    (3, \"Male\", 22, 1, \"Month-to-month\", 55.1, 551.0, 1),\n",
        "    (4, \"Female\", 58, 7, \"Two year\", 95.4, 6678.0, 0),\n",
        "    (5, \"Male\", 30, 3, \"Month-to-month\", 70.3, 2109.0, 1)\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"CustomerID\", IntegerType(), True),\n",
        "    StructField(\"Gender\", StringType(), True),\n",
        "    StructField(\"Age\", IntegerType(), True),\n",
        "    StructField(\"Tenure\", IntegerType(), True),\n",
        "    StructField(\"Contract\", StringType(), True),\n",
        "    StructField(\"MonthlyCharges\", DoubleType(), True),\n",
        "    StructField(\"TotalCharges\", DoubleType(), True),\n",
        "    StructField(\"Churn\", IntegerType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "E76FEARb0psb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThCwePjg2pz6",
        "outputId": "8f2886b0-ab97-49ab-dbd9-8567256107eb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---+------+--------------+--------------+------------+-----+\n",
            "|CustomerID|Gender|Age|Tenure|      Contract|MonthlyCharges|TotalCharges|Churn|\n",
            "+----------+------+---+------+--------------+--------------+------------+-----+\n",
            "|         1|  Male| 34|     2|Month-to-month|          65.5|      1300.0|    0|\n",
            "|         2|Female| 45|    12|      One year|          85.2|     10224.0|    0|\n",
            "|         3|  Male| 22|     1|Month-to-month|          55.1|       551.0|    1|\n",
            "|         4|Female| 58|     7|      Two year|          95.4|      6678.0|    0|\n",
            "|         5|  Male| 30|     3|Month-to-month|          70.3|      2109.0|    1|\n",
            "+----------+------+---+------+--------------+--------------+------------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Preprocessing:\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"Index\") for col in [\"Gender\", \"Contract\"]]\n",
        "assembler = VectorAssembler(inputCols=[\"GenderIndex\", \"Age\", \"Tenure\", \"ContractIndex\", \"MonthlyCharges\", \"TotalCharges\"], outputCol=\"features\")\n",
        "pipeline = Pipeline(stages=indexers + [assembler])\n",
        "df = pipeline.fit(df).transform(df)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpI2Fxik0zFv",
        "outputId": "bc1e0722-6eaf-423c-f8e8-42237c42796a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---+------+--------------+--------------+------------+-----+-----------+-------------+--------------------+\n",
            "|CustomerID|Gender|Age|Tenure|      Contract|MonthlyCharges|TotalCharges|Churn|GenderIndex|ContractIndex|            features|\n",
            "+----------+------+---+------+--------------+--------------+------------+-----+-----------+-------------+--------------------+\n",
            "|         1|  Male| 34|     2|Month-to-month|          65.5|      1300.0|    0|        0.0|          0.0|[0.0,34.0,2.0,0.0...|\n",
            "|         2|Female| 45|    12|      One year|          85.2|     10224.0|    0|        1.0|          1.0|[1.0,45.0,12.0,1....|\n",
            "|         3|  Male| 22|     1|Month-to-month|          55.1|       551.0|    1|        0.0|          0.0|[0.0,22.0,1.0,0.0...|\n",
            "|         4|Female| 58|     7|      Two year|          95.4|      6678.0|    0|        1.0|          2.0|[1.0,58.0,7.0,2.0...|\n",
            "|         5|  Male| 30|     3|Month-to-month|          70.3|      2109.0|    1|        0.0|          0.0|[0.0,30.0,3.0,0.0...|\n",
            "+----------+------+---+------+--------------+--------------+------------+-----+-----------+-------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Feature Scaling:\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
        "scalerModel = scaler.fit(df)\n",
        "df = scalerModel.transform(df)\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5T_G0BcRmRM",
        "outputId": "fdc08ce6-11d3-4fe1-8cc4-97228254ea79"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+---+------+--------------+--------------+------------+-----+-----------+-------------+--------------------+--------------------+\n",
            "|CustomerID|Gender|Age|Tenure|      Contract|MonthlyCharges|TotalCharges|Churn|GenderIndex|ContractIndex|            features|      scaledFeatures|\n",
            "+----------+------+---+------+--------------+--------------+------------+-----+-----------+-------------+--------------------+--------------------+\n",
            "|         1|  Male| 34|     2|Month-to-month|          65.5|      1300.0|    0|        0.0|          0.0|[0.0,34.0,2.0,0.0...|[-0.7302967433402...|\n",
            "|         2|Female| 45|    12|      One year|          85.2|     10224.0|    0|        1.0|          1.0|[1.0,45.0,12.0,1....|[1.09544511501033...|\n",
            "|         3|  Male| 22|     1|Month-to-month|          55.1|       551.0|    1|        0.0|          0.0|[0.0,22.0,1.0,0.0...|[-0.7302967433402...|\n",
            "|         4|Female| 58|     7|      Two year|          95.4|      6678.0|    0|        1.0|          2.0|[1.0,58.0,7.0,2.0...|[1.09544511501033...|\n",
            "|         5|  Male| 30|     3|Month-to-month|          70.3|      2109.0|    1|        0.0|          0.0|[0.0,30.0,3.0,0.0...|[-0.7302967433402...|\n",
            "+----------+------+---+------+--------------+--------------+------------+-----+-----------+-------------+--------------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Train-Test Split:\n",
        "train, test = df.randomSplit([0.9, 0.1], seed=42)"
      ],
      "metadata": {
        "id": "bn2oUXMU2g_H"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Model Evaluation:\n",
        "evaluator = BinaryClassificationEvaluator(labelCol=\"Churn\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "def evaluate_model(model, test_data):\n",
        "    predictions = model.transform(test_data)\n",
        "    accuracy = evaluator.evaluate(predictions)\n",
        "    print(f\"Model Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "q6uSK6Gp1O4y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "lr = LogisticRegression(featuresCol=\"scaledFeatures\", labelCol=\"Churn\")\n",
        "lr_model = lr.fit(train)\n",
        "\n",
        "# Evaluate Logistic Regression\n",
        "print(\"Logistic Regression:\")\n",
        "evaluate_model(lr_model, test)\n",
        "\n",
        "# Initialize and train Random Forest Classifier\n",
        "rf = RandomForestClassifier(featuresCol=\"scaledFeatures\", labelCol=\"Churn\")\n",
        "rf_model = rf.fit(train)\n",
        "\n",
        "# Evaluate Random Forest\n",
        "print(\"\\nRandom Forest:\")\n",
        "evaluate_model(rf_model, test)\n",
        "\n",
        "# Initialize and train Gradient-Boosted Trees Classifier\n",
        "gbt = GBTClassifier(featuresCol=\"scaledFeatures\", labelCol=\"Churn\", maxIter=10)\n",
        "gbt_model = gbt.fit(train)\n",
        "\n",
        "# Evaluate Gradient-Boosted Trees\n",
        "print(\"\\nGradient-Boosted Trees:\")\n",
        "evaluate_model(gbt_model, test)\n",
        "\n",
        "\n",
        "#Hyperparameter Tuning using CrossValidator (example with Random Forest)\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(rf.numTrees, [10, 20])\n",
        "             .addGrid(rf.maxDepth, [5, 10])\n",
        "             .build())\n",
        "\n",
        "crossval = CrossValidator(estimator=rf,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=2)  # Adjust numFolds as needed\n",
        "\n",
        "cvModel = crossval.fit(train)\n",
        "print(\"\\nRandom Forest with Cross Validation:\")\n",
        "evaluate_model(cvModel.bestModel, test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTcWDIwPT1b8",
        "outputId": "8ca392aa-3b9a-44f9-b8ad-bac253f514ed"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression:\n",
            "Model Accuracy: 0.50\n",
            "\n",
            "Random Forest:\n",
            "Model Accuracy: 0.50\n",
            "\n",
            "Gradient-Boosted Trees:\n",
            "Model Accuracy: 0.50\n",
            "\n",
            "Random Forest with Cross Validation:\n",
            "Model Accuracy: 0.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3rPCI9iGVjWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}