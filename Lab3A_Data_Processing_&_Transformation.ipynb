{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "yfoEAI8x2ycf",
      "metadata": {
        "id": "yfoEAI8x2ycf"
      },
      "source": [
        "# Lab 3A: Data Procesing and Transformation with PySpark"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6c43a5f",
      "metadata": {},
      "source": [
        "## Tasks\n",
        "\n",
        "1. **Introduction and Setup:**\n",
        "    - Provide an overview of the lab and its objectives.\n",
        "    - Import necessary libraries and initialize the Spark session.\n",
        "\n",
        "2. **Data Loading:**\n",
        "    - Load sample JSON data for customer details.\n",
        "    - Load sample Parquet data for customer details.\n",
        "    - Load sample CSV data for call records.\n",
        "\n",
        "3. **Data Cleaning:**\n",
        "    - Handle missing or empty values in customer data.\n",
        "    - Remove invalid call records (e.g., duration <= 0).\n",
        "\n",
        "4. **Data Transformation:**\n",
        "    - Explode complex data types (e.g., phone numbers).\n",
        "    - Add a region based on location.\n",
        "    - Calculate the total duration of calls by caller_id.\n",
        "    - Join customer data with total call durations.\n",
        "\n",
        "5. **Working with Parquet Data:**\n",
        "    - Write and read Parquet files.\n",
        "\n",
        "6. **Complex JSON Data Handling:**\n",
        "    - Handle nested JSON for detailed telecom metadata.\n",
        "    - Explode nested arrays.\n",
        "\n",
        "7. **Practice Case Study:**\n",
        "    - Implement data transformation and data processing on a custom telecom dataset.\n",
        "    - Add a region based on location.\n",
        "    - Explode phone numbers.\n",
        "    - Aggregate data to find the number of customers per plan and region.\n",
        "\n",
        "8. **Additional Analysis:**\n",
        "    - Perform any additional analysis or transformations as required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1giWhFeOiE3e",
      "metadata": {
        "id": "1giWhFeOiE3e"
      },
      "source": [
        "## PySpark for Data Processing and Transformation: Reference Notes\n",
        "\n",
        "**I. Core Concepts:**\n",
        "\n",
        "* **Resilient Distributed Datasets (RDDs):**  The fundamental data structure in Spark. RDDs are immutable, fault-tolerant collections of elements that can be distributed across a cluster.  Operations on RDDs are lazily evaluated, meaning transformations are not executed until an action is performed.  Key characteristics include immutability, partitioning, and fault tolerance through lineage tracking.\n",
        "\n",
        "* **DataFrames and Datasets:** Higher-level abstractions built on top of RDDs. DataFrames represent data in a tabular format with named columns, similar to SQL tables or Pandas DataFrames. Datasets provide type safety and optimization benefits, especially for Java/Scala, while being compatible with DataFrame APIs.\n",
        "\n",
        "* **Spark SQL:** Enables querying DataFrames using SQL syntax, offering a familiar and powerful way to perform data analysis and manipulation.\n",
        "\n",
        "* **Spark Streaming:**  Processes live data streams in near real-time.  Data streams are ingested, transformed, and analyzed as they arrive.\n",
        "\n",
        "* **MLlib (Spark Machine Learning Library):** A comprehensive set of machine learning algorithms and utilities for building and deploying machine learning models.  Supports various tasks, including classification, regression, clustering, and dimensionality reduction.\n",
        "\n",
        "* **GraphX:**  A library for graph processing, enabling operations on graph-structured data.\n",
        "\n",
        "\n",
        "**II. Data Processing Operations:**\n",
        "\n",
        "* **Transformations:** Operations that create new RDDs, DataFrames, or Datasets from existing ones without triggering immediate computation. Examples include `map`, `filter`, `flatMap`, `join`, `union`, `groupBy`, etc. Transformations are lazily evaluated.\n",
        "\n",
        "* **Actions:** Operations that trigger the execution of transformations and produce a result. Examples include `count`, `collect`, `take`, `first`, `saveAsTextFile`, etc. Actions initiate the actual computation.\n",
        "\n",
        "\n",
        "**III. Data Transformation Techniques:**\n",
        "\n",
        "* **Data Cleaning:** Handling missing values, outliers, and inconsistent data. Techniques include imputation, removal, or transformation of problematic data points.\n",
        "\n",
        "* **Data Aggregation:** Combining data to compute summary statistics. This often involves `groupBy` operations followed by aggregate functions like `sum`, `mean`, `min`, `max`, `count`, etc.\n",
        "\n",
        "* **Data Filtering:** Selecting data based on specific criteria. This is typically achieved with `filter` operations and conditional statements.\n",
        "\n",
        "* **Data Joining:** Combining data from multiple sources based on a common key.  Various types of joins (inner, outer, left, right) are available to handle different join scenarios.\n",
        "\n",
        "* **Data Pivoting/Reshaping:**  Changing the structure of the data.  This may involve converting rows to columns or vice versa.\n",
        "\n",
        "* **Feature Engineering:** Creating new features from existing data to improve the performance of machine learning models.\n",
        "\n",
        "* **Data Type Conversion:** Transforming data to the appropriate types required by downstream operations or models.\n",
        "\n",
        "* **Window Functions:** Performing calculations over a specific set of rows related to the current row.  Useful for calculating running totals, moving averages, or other time-series operations.\n",
        "\n",
        "* **UDFs (User-Defined Functions):** Extending Sparkâ€™s functionality by defining custom functions to process data.  UDFs can be used in transformations and SQL queries.\n",
        "\n",
        "**IV. Performance Optimization:**\n",
        "\n",
        "* **Data Partitioning:**  Strategically distributing data across the cluster to optimize data locality and reduce communication overhead.\n",
        "\n",
        "* **Caching:** Storing frequently accessed RDDs or DataFrames in memory for faster retrieval.\n",
        "\n",
        "* **Data Serialization:** Choosing efficient serialization formats to minimize data transfer times.\n",
        "\n",
        "* **Broadcast Variables:** Distributing read-only data to all nodes in a cluster.\n",
        "\n",
        "* **Accumulator Variables:** Aggregating values from different nodes in a cluster.\n",
        "\n",
        "\n",
        "\n",
        "**V. Integration with Other Systems:**\n",
        "\n",
        "* PySpark supports seamless integration with various data sources, including HDFS, AWS S3, databases, and other distributed storage systems.  Connectors are readily available for many popular databases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-YqvG79A5oyV",
      "metadata": {
        "id": "-YqvG79A5oyV"
      },
      "source": [
        "#PySpark Data Processing and Transformation in Telecom Domain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff7ef659",
      "metadata": {
        "id": "ff7ef659"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, regexp_replace, when, split, lit, struct, from_json\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "onBxN-ji5xGk",
      "metadata": {
        "id": "onBxN-ji5xGk"
      },
      "source": [
        "#Initialize Spark Session\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gzfo8E5J5vb-",
      "metadata": {
        "id": "gzfo8E5J5vb-"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder.appName(\"Telecom Data Processing\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "w6urFs3m5ujq",
      "metadata": {
        "id": "w6urFs3m5ujq"
      },
      "source": [
        "#Loading Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kPfEhv9m58E0",
      "metadata": {
        "id": "kPfEhv9m58E0"
      },
      "outputs": [],
      "source": [
        "#Sample JSON Data (Customer Details)\n",
        "\n",
        "customer_data = [\n",
        "    '{\"customer_id\": \"C001\", \"name\": \"Alice\", \"location\": \"NY\", \"phone_numbers\": [\"1234567890\", \"0987654321\"]}',\n",
        "    '{\"customer_id\": \"C002\", \"name\": \"Bob\", \"location\": \"CA\", \"phone_numbers\": [\"1234509876\"]}',\n",
        "    '{\"customer_id\": \"C003\", \"name\": \"Charlie\", \"location\": \"TX\", \"phone_numbers\": []}',\n",
        "    '{\"customer_id\": \"C004\", \"name\": \"David\", \"location\": \"FL\", \"phone_numbers\": [\"9876543210\"]}',\n",
        "    '{\"customer_id\": \"C005\", \"name\": \"Eve\", \"location\": \"NY\", \"phone_numbers\": [\"1122334455\", \"5566778899\"]}',\n",
        "    '{\"customer_id\": \"C006\", \"name\": \"Frank\", \"location\": \"CA\", \"phone_numbers\": []}',\n",
        "    '{\"customer_id\": \"C007\", \"name\": \"Grace\", \"location\": \"TX\", \"phone_numbers\": [\"9988776655\"]}',\n",
        "    '{\"customer_id\": \"C008\", \"name\": \"Hank\", \"location\": \"NY\", \"phone_numbers\": [\"2233445566\"]}',\n",
        "    '{\"customer_id\": \"C009\", \"name\": \"Ivy\", \"location\": \"CA\", \"phone_numbers\": [\"3344556677\"]}',\n",
        "    '{\"customer_id\": \"C010\", \"name\": \"Jack\", \"location\": \"TX\", \"phone_numbers\": [\"4455667788\", \"7788990011\"]}'\n",
        "]\n",
        "\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"phone_numbers\", ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "customer_df = spark.read.json(spark.sparkContext.parallelize(customer_data), schema=customer_schema)\n",
        "customer_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8tD7xoU6GlA",
      "metadata": {
        "id": "b8tD7xoU6GlA"
      },
      "outputs": [],
      "source": [
        "#Sample Parquet Data (Customer Details)\n",
        "customer_df.write.mode(\"overwrite\").parquet(\"customer_data.parquet\")\n",
        "customer_parquet_df = spark.read.parquet(\"customer_data.parquet\")\n",
        "customer_parquet_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JQZrwj9k6V0j",
      "metadata": {
        "id": "JQZrwj9k6V0j"
      },
      "outputs": [],
      "source": [
        "# #### Sample CSV Data (Call Records)\n",
        "call_records_data = [\n",
        "    \"caller_id,receiver_id,duration,status\",\n",
        "    \"C001,C002,5,Connected\",\n",
        "    \"C002,C003,15,Connected\",\n",
        "    \"C003,C001,7,Dropped\",\n",
        "    \"C001,C003,10,Connected\",\n",
        "    \"C002,C001,0,Failed\",\n",
        "    \"C004,C005,12,Connected\",\n",
        "    \"C005,C006,20,Connected\",\n",
        "    \"C006,C007,3,Dropped\",\n",
        "    \"C007,C008,25,Connected\",\n",
        "    \"C008,C009,8,Connected\",\n",
        "    \"C009,C010,30,Connected\",\n",
        "    \"C010,C001,14,Connected\",\n",
        "    \"C002,C004,0,Failed\",\n",
        "    \"C003,C005,18,Connected\",\n",
        "    \"C006,C008,6,Connected\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wkibf54d6W49",
      "metadata": {
        "id": "wkibf54d6W49"
      },
      "outputs": [],
      "source": [
        "call_records_rdd = spark.sparkContext.parallelize(call_records_data)\n",
        "call_records_df = spark.read.option(\"header\", True).csv(call_records_rdd)\n",
        "call_records_df = call_records_df.withColumn(\"duration\", col(\"duration\").cast(IntegerType()))\n",
        "call_records_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mTC97l3f6gxe",
      "metadata": {
        "id": "mTC97l3f6gxe"
      },
      "source": [
        "#Data Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xv_2Lmzq6Z3m",
      "metadata": {
        "id": "xv_2Lmzq6Z3m"
      },
      "outputs": [],
      "source": [
        "#Handling Missing or Empty Values\n",
        "customer_df_cleaned = customer_df.withColumn(\n",
        "    \"phone_numbers\", when(col(\"phone_numbers\").isNull() | (col(\"phone_numbers\").isNull()) | (col(\"phone_numbers\")==lit(\"\")), lit(None)).otherwise(col(\"phone_numbers\"))\n",
        ")\n",
        "customer_df_cleaned.show(truncate=False)\n",
        "\n",
        "customer_parquet_df_cleaned = customer_parquet_df.withColumn(\n",
        "    \"phone_numbers\", when(col(\"phone_numbers\").isNull() | (col(\"phone_numbers\").isNull()) | (col(\"phone_numbers\")==lit(\"\")), lit(None)).otherwise(col(\"phone_numbers\"))\n",
        ")\n",
        "customer_parquet_df_cleaned.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BEAKfGcJ6Z0L",
      "metadata": {
        "id": "BEAKfGcJ6Z0L"
      },
      "outputs": [],
      "source": [
        "#Removing Invalid Call Records (e.g., duration <= 0)\n",
        "valid_calls_df = call_records_df.filter(col(\"duration\") > 0)\n",
        "valid_calls_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "i5IQt_uw7Zws",
      "metadata": {
        "id": "i5IQt_uw7Zws"
      },
      "source": [
        "#Data Transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vtATbpAE6pGD",
      "metadata": {
        "id": "vtATbpAE6pGD"
      },
      "outputs": [],
      "source": [
        "#Exploding Complex Data Types\n",
        "exploded_customer_df = customer_df_cleaned.withColumn(\"phone_number\", explode(col(\"phone_numbers\")))\n",
        "exploded_customer_df.show(truncate=False)\n",
        "\n",
        "exploded_customer_parquet_df = customer_parquet_df_cleaned.withColumn(\"phone_number\", explode(col(\"phone_numbers\")))\n",
        "exploded_customer_parquet_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fKT4Zwlg6pCf",
      "metadata": {
        "id": "fKT4Zwlg6pCf"
      },
      "outputs": [],
      "source": [
        "# Adding a region based on location.\n",
        "region_mapping_expr = when(col(\"location\") == \"NY\", \"North-East\").when(col(\"location\") == \"CA\", \"West\").when(col(\"location\") == \"TX\", \"South\").when(col(\"location\") == \"FL\", \"South-East\").otherwise(\"Unknown\")\n",
        "\n",
        "enriched_customer_df = customer_df_cleaned.withColumn(\"region\", region_mapping_expr)\n",
        "enriched_customer_df.show(truncate=False)\n",
        "\n",
        "enriched_customer_parquet_df = customer_parquet_df_cleaned.withColumn(\"region\", region_mapping_expr)\n",
        "enriched_customer_parquet_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KGPn2d36DdpG",
      "metadata": {
        "id": "KGPn2d36DdpG"
      },
      "outputs": [],
      "source": [
        "# Total duration of calls by caller_id.\n",
        "total_call_duration_df = valid_calls_df.groupBy(\"caller_id\").sum(\"duration\").withColumnRenamed(\"sum(duration)\", \"total_duration\")\n",
        "total_call_duration_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010OlVbRDjSu",
      "metadata": {
        "id": "010OlVbRDjSu"
      },
      "outputs": [],
      "source": [
        "# Joining customer data with total call durations.\n",
        "final_df = enriched_customer_df.join(total_call_duration_df, enriched_customer_df.customer_id == total_call_duration_df.caller_id, \"left_outer\").drop(\"caller_id\")\n",
        "final_df = final_df.withColumn(\"total_duration\", when(col(\"total_duration\").isNull(), lit(0)).otherwise(col(\"total_duration\")))\n",
        "final_df.show(truncate=False)\n",
        "\n",
        "final_parquet_df = enriched_customer_parquet_df.join(total_call_duration_df, enriched_customer_parquet_df.customer_id == total_call_duration_df.caller_id, \"left_outer\").drop(\"caller_id\")\n",
        "final_parquet_df = final_parquet_df.withColumn(\"total_duration\", when(col(\"total_duration\").isNull(), lit(0)).otherwise(col(\"total_duration\")))\n",
        "final_parquet_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rIxtwWjpDwZq",
      "metadata": {
        "id": "rIxtwWjpDwZq"
      },
      "source": [
        "#Working with Parquet Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1TssTdyyDt6t",
      "metadata": {
        "id": "1TssTdyyDt6t"
      },
      "outputs": [],
      "source": [
        "# Writing and Reading Parquet Files\n",
        "final_df.write.mode(\"overwrite\").parquet(\"telecom_data_final.parquet\")\n",
        "final_parquet_df.write.mode(\"overwrite\").parquet(\"telecom_data_final_parquet.parquet\")\n",
        "\n",
        "parquet_df = spark.read.parquet(\"telecom_data_final.parquet\")\n",
        "parquet_df.show(truncate=False)\n",
        "\n",
        "parquet_df2 = spark.read.parquet(\"telecom_data_final_parquet.parquet\")\n",
        "parquet_df2.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CdPf_6CTD5c6",
      "metadata": {
        "id": "CdPf_6CTD5c6"
      },
      "source": [
        "#Complex JSON Data Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Nl5bC5qSDxvo",
      "metadata": {
        "id": "Nl5bC5qSDxvo"
      },
      "outputs": [],
      "source": [
        "# Handling Nested JSON for Detailed Telecom Metadata\n",
        "nested_json_data = [\n",
        "    '{\"network_id\": \"N001\", \"details\": {\"type\": \"4G\", \"bandwidth\": \"100Mbps\"}, \"locations\": [\"NY\", \"CA\"]}',\n",
        "    '{\"network_id\": \"N002\", \"details\": {\"type\": \"5G\", \"bandwidth\": \"1Gbps\"}, \"locations\": [\"TX\", \"FL\"]}',\n",
        "    '{\"network_id\": \"N003\", \"details\": {\"type\": \"3G\", \"bandwidth\": \"20Mbps\"}, \"locations\": [\"CA\", \"NY\"]}'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3CSo0mKPD-YN",
      "metadata": {
        "id": "3CSo0mKPD-YN"
      },
      "outputs": [],
      "source": [
        "nested_schema = StructType([\n",
        "    StructField(\"network_id\", StringType(), True),\n",
        "    StructField(\"details\", StructType([\n",
        "        StructField(\"type\", StringType(), True),\n",
        "        StructField(\"bandwidth\", StringType(), True)\n",
        "    ]), True),\n",
        "    StructField(\"locations\", ArrayType(StringType()), True)\n",
        "])\n",
        "\n",
        "nested_json_df = spark.read.json(spark.sparkContext.parallelize(nested_json_data), schema=nested_schema)\n",
        "nested_json_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JXzT7GXoECbK",
      "metadata": {
        "id": "JXzT7GXoECbK"
      },
      "outputs": [],
      "source": [
        "# Exploding Nested Arrays\n",
        "exploded_network_df = nested_json_df.withColumn(\"location\", explode(col(\"locations\"))).drop(\"locations\")\n",
        "exploded_network_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AToFgN66kCUP",
      "metadata": {
        "id": "AToFgN66kCUP"
      },
      "source": [
        "#Practice Case study - Implementing data transformation and data processing on custom telecom dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VBksdbxLkM7B",
      "metadata": {
        "id": "VBksdbxLkM7B"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "#customer_data = [\n",
        "    '{\"customer_id\": \"C001\", \"name\": \"Alice\", \"location\": \"NY\", \"phone_numbers\": [\"1234567890\", \"0987654321\"], \"age\": 30, \"plan\": \"Premium\"}',\n",
        "    '{\"customer_id\": \"C002\", \"name\": \"Bob\", \"location\": \"CA\", \"phone_numbers\": [\"1234509876\"], \"age\": 25, \"plan\": \"Basic\"}',\n",
        "    '{\"customer_id\": \"C003\", \"name\": \"Charlie\", \"location\": \"TX\", \"phone_numbers\": [], \"age\": 40, \"plan\": \"Premium\"}',\n",
        "    '{\"customer_id\": \"C004\", \"name\": \"David\", \"location\": \"FL\", \"phone_numbers\": [\"9876543210\"], \"age\": null, \"plan\": \"Basic\"}', # Example of null value\n",
        "    '{\"customer_id\": \"C005\", \"name\": \"Eve\", \"location\": \"NY\", \"phone_numbers\": [\"1122334455\", \"5566778899\"], \"age\": 35, \"plan\": \"Premium\"}',\n",
        "    '{\"customer_id\": \"C006\", \"name\": \"Frank\", \"location\": \"CA\", \"phone_numbers\": [], \"age\": 28, \"plan\":\"Basic\"}'\n",
        "]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k1AQzIctmRLZ",
      "metadata": {
        "id": "k1AQzIctmRLZ"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, explode, regexp_replace, when, split, lit, struct, from_json\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType, MapType\n",
        "import json\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Telecom Data Processing\").getOrCreate()\n",
        "\n",
        "# Define the schema using imported StructType and other types\n",
        "customer_schema = StructType([\n",
        "    StructField(\"customer_id\", StringType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"location\", StringType(), True),\n",
        "    StructField(\"phone_numbers\", ArrayType(StringType()), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"plan\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Define the data\n",
        "customer_data_processed = [ # Assign the data to the customer_data_processed variable\n",
        "    {\"customer_id\": \"C001\", \"name\": \"Alice\", \"location\": \"NY\", \"phone_numbers\": [\"1234567890\", \"0987654321\"], \"age\": 30, \"plan\": \"Premium\"},\n",
        "    {\"customer_id\": \"C002\", \"name\": \"Bob\", \"location\": \"CA\", \"phone_numbers\": [\"1234509876\"], \"age\": 25, \"plan\": \"Basic\"},\n",
        "    {\"customer_id\": \"C003\", \"name\": \"Charlie\", \"location\": \"TX\", \"phone_numbers\": [], \"age\": 40, \"plan\": \"Premium\"},\n",
        "    {\"customer_id\": \"C004\", \"name\": \"David\", \"location\": \"FL\", \"phone_numbers\": [\"9876543210\"], \"age\": None, \"plan\": \"Basic\"},  # Example of null value\n",
        "    {\"customer_id\": \"C005\", \"name\": \"Eve\", \"location\": \"NY\", \"phone_numbers\": [\"1122334455\", \"5566778899\"], \"age\": 35, \"plan\": \"Premium\"},\n",
        "    {\"customer_id\": \"C006\", \"name\": \"Frank\", \"location\": \"CA\", \"phone_numbers\": [], \"age\": 28, \"plan\": \"Basic\"}\n",
        "]\n",
        "\n",
        "customer_df = spark.createDataFrame(customer_data_processed, schema=customer_schema)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hNwiCJr_k72z",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNwiCJr_k72z",
        "outputId": "2ab3575c-93a0-468b-fdf0-909449cfb5d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-------+--------+------------------------+----+-------+----------+\n",
            "|customer_id|name   |location|phone_numbers           |age |plan   |region    |\n",
            "+-----------+-------+--------+------------------------+----+-------+----------+\n",
            "|C001       |Alice  |NY      |[1234567890, 0987654321]|30  |Premium|North-East|\n",
            "|C002       |Bob    |CA      |[1234509876]            |25  |Basic  |West      |\n",
            "|C003       |Charlie|TX      |[]                      |40  |Premium|South     |\n",
            "|C004       |David  |FL      |[9876543210]            |NULL|Basic  |South-East|\n",
            "|C005       |Eve    |NY      |[1122334455, 5566778899]|35  |Premium|North-East|\n",
            "|C006       |Frank  |CA      |[]                      |28  |Basic  |West      |\n",
            "+-----------+-------+--------+------------------------+----+-------+----------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Transformation: Adding a region based on location\n",
        "region_mapping_expr = when(col(\"location\") == \"NY\", \"North-East\") \\\n",
        "                      .when(col(\"location\") == \"CA\", \"West\") \\\n",
        "                      .when(col(\"location\") == \"TX\", \"South\") \\\n",
        "                      .when(col(\"location\") == \"FL\", \"South-East\") \\\n",
        "                      .otherwise(\"Unknown\")\n",
        "\n",
        "customer_df = customer_df.withColumn(\"region\", region_mapping_expr)\n",
        "\n",
        "customer_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6DCUV9k7z8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a6DCUV9k7z8",
        "outputId": "4b896316-18b3-4326-b0d4-8499b5f8a480"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----------+-----+--------+------------------------+----+-------+----------+------------+\n",
            "|customer_id|name |location|phone_numbers           |age |plan   |region    |phone_number|\n",
            "+-----------+-----+--------+------------------------+----+-------+----------+------------+\n",
            "|C001       |Alice|NY      |[1234567890, 0987654321]|30  |Premium|North-East|1234567890  |\n",
            "|C001       |Alice|NY      |[1234567890, 0987654321]|30  |Premium|North-East|0987654321  |\n",
            "|C002       |Bob  |CA      |[1234509876]            |25  |Basic  |West      |1234509876  |\n",
            "|C004       |David|FL      |[9876543210]            |NULL|Basic  |South-East|9876543210  |\n",
            "|C005       |Eve  |NY      |[1122334455, 5566778899]|35  |Premium|North-East|1122334455  |\n",
            "|C005       |Eve  |NY      |[1122334455, 5566778899]|35  |Premium|North-East|5566778899  |\n",
            "+-----------+-----+--------+------------------------+----+-------+----------+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Transformation: Explode phone numbers\n",
        "exploded_customer_df = customer_df.withColumn(\"phone_number\", explode(col(\"phone_numbers\")))\n",
        "exploded_customer_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XuV2ThXMk7xR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuV2ThXMk7xR",
        "outputId": "9a06eebd-2298-43b5-97be-3cd095a73e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+----------+-----+\n",
            "|   plan|    region|count|\n",
            "+-------+----------+-----+\n",
            "|Premium|North-East|    2|\n",
            "|Premium|     South|    1|\n",
            "|  Basic|      West|    2|\n",
            "|  Basic|South-East|    1|\n",
            "+-------+----------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Data Aggregation: Number of customers per plan and region\n",
        "customer_plan_region_df = customer_df.groupBy(\"plan\", \"region\").count()\n",
        "customer_plan_region_df.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
